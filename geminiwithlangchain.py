import streamlit as st
import pandas as pd
import os
# from google import genai # No longer directly needed for LLM calls
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_ollama import OllamaEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain_core.messages import HumanMessage, AIMessage

# --- Configuration ---
# It's best to use Streamlit secrets for API keys
# For local development, you can set it as an environment variable
# or directly here (less secure for shared code)
# GOOGLE_API_KEY = st.secrets.get("GOOGLE_API_KEY", "AIzaSyA-bjwh-ZfB9kFv9W5pPNPGWvZA8iNO43I") # Replace with your key or set in st.secrets
# if GOOGLE_API_KEY == "AIzaSyA-bjwh-ZfB9kFv9W5pPNPGWvZA8iNO43I" and not os.getenv("GOOGLE_API_KEY"):
#     st.error("Please provide your Google API Key either in st.secrets or as an environment variable.")
#     st.stop()
# elif not GOOGLE_API_KEY and os.getenv("GOOGLE_API_KEY"):
#     GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

GOOGLE_API_KEY = "AIzaSyA-bjwh-ZfB9kFv9W5pPNPGWvZA8iNO43I"  # Replace with your actual key or set in st.secrets

st.set_page_config(page_title="RAG Chatbot - Restaurant Reviews", page_icon="üçΩÔ∏è")

# Initialize chat history in session state (for display)
if "chat_history_display" not in st.session_state:
    st.session_state.chat_history_display = []

# Initialize Langchain memory in session state
if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True, # Important for LLMs that expect message objects
        output_key='answer' # To ensure the LLM's response is stored correctly in memory
    )

# ------------------ Langchain LLM Client ------------------
# Note: "gemini-2.0-flash" might not be a direct model name in Langchain's wrapper.
# Common options are "gemini-pro", "gemini-1.0-pro", "gemini-1.5-flash-latest", "gemini-1.5-pro-latest".
# Let's use "gemini-pro" as a widely available option, or "gemini-1.5-flash-latest" if you prefer flash.
try:
    llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", google_api_key=GOOGLE_API_KEY)
except Exception as e:
    st.error(f"Error initializing Google LLM: {e}")
    st.stop()

# ------------------ Load Data & Embedding ------------------
@st.cache_resource
def load_vector_store():
    try:
        df = pd.read_csv("realistic_restaurant_reviews.csv")
    except FileNotFoundError:
        st.error("Error: 'realistic_restaurant_reviews.csv' not found. Please make sure the file exists in the same directory.")
        st.stop()
        return None # Ensure function returns None if file not found

    embeddings = OllamaEmbeddings(model="mxbai-embed-large")
    db_location = "./chroma_langchain_db"
    # Check if the Chroma DB directory exists and has files.
    # A more robust check might be to see if key files like 'chroma.sqlite3' exist.
    add_documents = not (os.path.exists(db_location) and os.listdir(db_location))


    documents = [] # ids are not explicitly needed for add_documents with Chroma default behavior
    if add_documents:
        st.info("No existing vector store found or it's empty. Creating and adding documents...")
        for i, row in df.iterrows():
            content = (
                f"Title: {row['Title']}\n"
                f"Review: {row['Review']}\n"
                f"Rating: {row['Rating']}\n"
                f"Date: {row['Date']}"
            )
            # Langchain Document does not take 'id' directly in constructor in newer versions
            # It will be generated by Chroma or can be passed to `add_documents` via an `ids` list
            doc = Document(
                page_content=content,
                metadata={"rating": row["Rating"], "date": row["Date"], "source_id": str(i)},
            )
            documents.append(doc)

    vector_store = Chroma(
        collection_name="restaurant_reviews",
        persist_directory=db_location,
        embedding_function=embeddings,
    )

    if add_documents and documents:
        # Generate IDs if you want to control them, otherwise Chroma handles it
        ids_to_add = [str(i) for i in range(len(documents))]
        vector_store.add_documents(documents=documents, ids=ids_to_add)
        vector_store.persist() # Ensure persistence after adding
        st.success(f"Added {len(documents)} documents to the vector store.")
    elif not add_documents:
        st.info("Loaded existing vector store.")

    return vector_store.as_retriever(search_kwargs={"k": 5})

retriever = load_vector_store()
if retriever is None: # Handle case where load_vector_store failed
    st.stop()

# ------------------ ConversationalRetrievalChain ------------------
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=st.session_state.memory,
    return_source_documents=True, # To get the retrieved documents
    verbose=False # Set to True for debugging
)

# ------------------ Streamlit UI ------------------
st.title("üçΩÔ∏è Restaurant Review Assistant (Langchain)")
st.markdown("Ask any question based on real restaurant reviews!")

st.markdown("## üí¨ Chat History")
# Display chat history from st.session_state.chat_history_display
for i, (q, a) in enumerate(st.session_state.chat_history_display):
    with st.chat_message("user", avatar="‚ùì"):
        st.markdown(q)
    with st.chat_message("assistant", avatar="ü§ñ"):
        st.markdown(a)

question = st.chat_input("Enter your question:")

if question:
    st.chat_message("user", avatar="‚ùì").markdown(question)

    with st.spinner("Searching and thinking..."):
        try:
            # The chain uses the memory automatically.
            # The input to invoke is just the new question.
            result = qa_chain.invoke({"question": question})
            answer = result["answer"]
            retrieved_docs = result["source_documents"]

            # Add to display history
            st.session_state.chat_history_display.append((question, answer))

            with st.chat_message("assistant", avatar="ü§ñ"):
                st.markdown(answer)

            with st.expander("üîç Show retrieved reviews"):
                if retrieved_docs:
                    for i, doc in enumerate(retrieved_docs):
                        st.markdown(f"**Review #{i+1} (Source ID: {doc.metadata.get('source_id', 'N/A')})**")
                        st.markdown(doc.page_content.replace("\n", "<br>"), unsafe_allow_html=True)
                        st.markdown(f"*Metadata: {doc.metadata}*")
                else:
                    st.write("No reviews were retrieved for this query.")

        except Exception as e:
            st.error(f"An error occurred: {e}")
            # Optionally, log the full traceback for debugging
            # import traceback
            # st.error(traceback.format_exc())